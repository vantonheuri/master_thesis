{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d0bb5c",
   "metadata": {},
   "source": [
    "# 1. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9514ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"ALPHA_VANTAGE_API_KEY\")\n",
    "BASE_URL = 'https://www.alphavantage.co/query'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebcf80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from Alpha Vantage (general use)\n",
    "def fetch_data(params, is_csv=False):\n",
    "    params['apikey'] = API_KEY\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    if is_csv:\n",
    "        return response.text\n",
    "    try:\n",
    "        return response.json()\n",
    "    except ValueError:\n",
    "        return {}\n",
    "\n",
    "def ensure_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def save_json(data, base_dir, filename, key=None):\n",
    "    if key:\n",
    "        data = data.get(key, {})\n",
    "    if not data:\n",
    "        print(f\"‚ùå No data for {filename}\")\n",
    "        return\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df.index.name = 'date'\n",
    "    full_path = os.path.join(base_dir, filename)\n",
    "    df.to_csv(full_path)\n",
    "    print(f\"‚úÖ Saved: {full_path}\")\n",
    "\n",
    "def save_flat_json(data, base_dir, filename):\n",
    "    if not data:\n",
    "        print(f\"‚ùå No data for {filename}\")\n",
    "        return\n",
    "    df = pd.DataFrame([data])\n",
    "    full_path = os.path.join(base_dir, filename)\n",
    "    df.to_csv(full_path, index=False)\n",
    "    print(f\"‚úÖ Saved: {full_path}\")\n",
    "\n",
    "def save_csv(content, base_dir, filename):\n",
    "    full_path = os.path.join(base_dir, filename)\n",
    "    with open(full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"‚úÖ Saved: {full_path}\")\n",
    "\n",
    "def fetch_stock_data(symbol):\n",
    "    base_dir = f\"data/{symbol}\"\n",
    "    ensure_dir(base_dir)\n",
    "\n",
    "    fundamentals_dir = os.path.join(base_dir, \"fundamentals\")\n",
    "    prices_dir = os.path.join(base_dir, \"prices\")\n",
    "    quotes_dir = os.path.join(base_dir, \"quotes\")\n",
    "    news_dir = os.path.join(base_dir, \"news\")\n",
    "\n",
    "    for folder in [fundamentals_dir, prices_dir, quotes_dir, news_dir]:\n",
    "        ensure_dir(folder)\n",
    "\n",
    "    endpoints = [\n",
    "        (\"TIME_SERIES_DAILY\", \"prices\", \"daily.csv\", \"Time Series (Daily)\"),\n",
    "        (\"TIME_SERIES_DAILY_ADJUSTED\", \"prices\", \"daily_adjusted.csv\", \"Time Series (Daily)\"),\n",
    "        (\"TIME_SERIES_WEEKLY\", \"prices\", \"weekly.csv\", \"Weekly Time Series\"),\n",
    "        (\"TIME_SERIES_WEEKLY_ADJUSTED\", \"prices\", \"weekly_adjusted.csv\", \"Weekly Adjusted Time Series\"),\n",
    "        (\"TIME_SERIES_MONTHLY\", \"prices\", \"monthly.csv\", \"Monthly Time Series\"),\n",
    "        (\"TIME_SERIES_MONTHLY_ADJUSTED\", \"prices\", \"monthly_adjusted.csv\", \"Monthly Adjusted Time Series\"),\n",
    "        (\"GLOBAL_QUOTE\", \"quotes\", \"quote.csv\", None),\n",
    "        (\"OVERVIEW\", \"fundamentals\", \"overview.csv\", None),\n",
    "        (\"INCOME_STATEMENT\", \"fundamentals\", \"income_statement.csv\", None),\n",
    "        (\"BALANCE_SHEET\", \"fundamentals\", \"balance_sheet.csv\", None),\n",
    "        (\"CASH_FLOW\", \"fundamentals\", \"cash_flow.csv\", None),\n",
    "        (\"EARNINGS\", \"fundamentals\", \"earnings.csv\", None),\n",
    "        (\"DIVIDENDS\", \"fundamentals\", \"dividends.csv\", None),\n",
    "        (\"SPLITS\", \"fundamentals\", \"splits.csv\", None),\n",
    "        (\"INSIDER_TRANSACTIONS\", \"fundamentals\", \"insider_transactions.csv\", None)\n",
    "    ]\n",
    "\n",
    "    for func, folder, filename, key in endpoints:\n",
    "        params = {'function': func, 'symbol': symbol}\n",
    "\n",
    "        # Get full history for time series\n",
    "        if \"TIME_SERIES\" in func and \"INTRADAY\" not in func:\n",
    "            params[\"outputsize\"] = \"full\"\n",
    "\n",
    "        data = fetch_data(params)\n",
    "        full_dir = os.path.join(base_dir, folder)\n",
    "\n",
    "        if isinstance(data, dict):\n",
    "            if key:\n",
    "                save_json(data, full_dir, f\"{symbol}_{filename}\", key)\n",
    "            else:\n",
    "                if 'annualReports' in data:\n",
    "                    pd.DataFrame(data['annualReports']).to_csv(os.path.join(full_dir, f\"{symbol}_{filename}\"), index=False)\n",
    "                elif 'quarterlyEarnings' in data:\n",
    "                    pd.DataFrame(data['quarterlyEarnings']).to_csv(os.path.join(full_dir, f\"{symbol}_{filename}\"), index=False)\n",
    "                else:\n",
    "                    save_flat_json(data, full_dir, f\"{symbol}_{filename}\")\n",
    "        time.sleep(15)\n",
    "\n",
    "def fetch_news_sentiment(symbol):\n",
    "    news_dir = f\"data/{symbol}/news\"\n",
    "    ensure_dir(news_dir)\n",
    "    params = {'function': 'NEWS_SENTIMENT', 'tickers': symbol, 'limit': 1000}\n",
    "    data = fetch_data(params)\n",
    "    if data and \"feed\" in data:\n",
    "        df = pd.DataFrame(data[\"feed\"])\n",
    "        df.to_csv(os.path.join(news_dir, f\"{symbol}_news_sentiment.csv\"), index=False)\n",
    "        print(f\"‚úÖ Saved: {symbol}_news_sentiment.csv\")\n",
    "    time.sleep(15)\n",
    "\n",
    "def fetch_earnings_transcript(symbol, quarter=\"2024Q1\"):\n",
    "    fundamentals_dir = f\"data/{symbol}/fundamentals\"\n",
    "    ensure_dir(fundamentals_dir)\n",
    "    params = {'function': 'EARNINGS_CALL_TRANSCRIPT', 'symbol': symbol, 'quarter': quarter}\n",
    "    data = fetch_data(params)\n",
    "    save_flat_json(data, fundamentals_dir, f\"{symbol}_earnings_transcript.csv\")\n",
    "    time.sleep(15)\n",
    "\n",
    "def fetch_macro_data():\n",
    "    base_dir = \"data/macros\"\n",
    "    ensure_dir(base_dir)\n",
    "    indicators = [\n",
    "        \"REAL_GDP\", \"REAL_GDP_PER_CAPITA\", \"TREASURY_YIELD\", \"FEDERAL_FUNDS_RATE\", \"CPI\", \"INFLATION\",\n",
    "        \"RETAIL_SALES\", \"DURABLES\", \"UNEMPLOYMENT\", \"NONFARM_PAYROLL\"\n",
    "    ]\n",
    "    for ind in indicators:\n",
    "        params = {'function': ind}\n",
    "        data = fetch_data(params)\n",
    "        save_json(data, base_dir, f\"{ind.lower()}.csv\")\n",
    "        time.sleep(15)\n",
    "\n",
    "def fetch_calendars():\n",
    "    base_dir = \"data/macros\"\n",
    "    ensure_dir(base_dir)\n",
    "    endpoints = [\n",
    "        (\"EARNINGS_CALENDAR\", \"earnings_calendar.csv\", {\"horizon\": \"3month\"}),\n",
    "        (\"IPO_CALENDAR\", \"ipo_calendar.csv\", {})\n",
    "    ]\n",
    "    for func, filename, params in endpoints:\n",
    "        params.update({'function': func})\n",
    "        content = fetch_data(params, is_csv=True)\n",
    "        save_csv(content, base_dir, filename)\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496271d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    symbols = [\"AAPL\", \"AMZN\", \"GOOG\", \"MSFT\", \"META\", \"NVDA\", \"TSLA\", \"SPY\", \"URTH\"]\n",
    "    for symbol in symbols:\n",
    "        try:\n",
    "            print(f\"\\nüìà Fetching data for {symbol}...\")\n",
    "            fetch_stock_data(symbol)\n",
    "            fetch_news_sentiment(symbol)\n",
    "            fetch_earnings_transcript(symbol)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with {symbol}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\nüåç Fetching macroeconomic indicators...\")\n",
    "    fetch_macro_data()\n",
    "\n",
    "    print(\"\\nüìÜ Fetching IPO and Earnings calendars...\")\n",
    "    fetch_calendars()\n",
    "\n",
    "    print(\"\\nüéâ All data fetched successfully.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084a685",
   "metadata": {},
   "source": [
    "## Market Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ea9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar API Key desde .env\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"ALPHA_VANTAGE_API_KEY\")\n",
    "BASE_URL = 'https://www.alphavantage.co/query'\n",
    "\n",
    "# Lista de s√≠mbolos\n",
    "symbols = [ \"AAPL\", \"AMZN\", \"GOOGL\", \"MSFT\", \"META\", \"NVDA\", \"TSLA\"]\n",
    "\n",
    "# Funci√≥n para hacer request a Alpha Vantage\n",
    "def fetch_data(params):\n",
    "    params['apikey'] = API_KEY\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    try:\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Guardar el JSON plano en formato vertical clave-valor (una fila por clave)\n",
    "def save_flat_json_vertical(data, base_dir, filename):\n",
    "    if not data or \"Note\" in data or \"Error Message\" in data:\n",
    "        print(f\"‚ùå Error en la respuesta para {filename}\")\n",
    "        return False\n",
    "    df = pd.DataFrame(data.items(), columns=[\"Key\", \"Value\"])\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    path = os.path.join(base_dir, filename)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"‚úÖ Guardado: {path}\")\n",
    "    return True\n",
    "\n",
    "# Proceso principal\n",
    "def main():\n",
    "    for symbol in symbols:\n",
    "        print(f\"\\nüì• Descargando OVERVIEW para {symbol}...\")\n",
    "        success = False\n",
    "        attempts = 0\n",
    "\n",
    "        while not success and attempts < 3:\n",
    "            params = {'function': 'OVERVIEW', 'symbol': symbol}\n",
    "            data = fetch_data(params)\n",
    "            success = save_flat_json_vertical(data, f\"data/{symbol}/fundamentals\", f\"{symbol}_overview.csv\")\n",
    "            if not success:\n",
    "                print(f\"üîÅ Esperando para reintentar {symbol}...\")\n",
    "                time.sleep(20)\n",
    "            attempts += 1\n",
    "\n",
    "        time.sleep(15)  # Tiempo de espera entre s√≠mbolos para no superar el l√≠mite gratuito\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Descargando news sentiment para GOOGL...\n",
      "‚ùå No se encontraron datos v√°lidos para GOOGL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Cargar API Key\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"ALPHA_VANTAGE_API_KEY\")\n",
    "BASE_URL = 'https://www.alphavantage.co/query'\n",
    "\n",
    "# Funci√≥n general de descarga\n",
    "def fetch_data(params):\n",
    "    params['apikey'] = API_KEY\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    try:\n",
    "        return response.json()\n",
    "    except ValueError:\n",
    "        return {}\n",
    "\n",
    "# Asegurar carpeta\n",
    "def ensure_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Funci√≥n aislada para GOOGL news\n",
    "def fetch_news_sentiment(symbol=\"GOOGL\"):\n",
    "    news_dir = f\"data/{symbol}/news\"\n",
    "    ensure_dir(news_dir)\n",
    "\n",
    "    print(f\"üì• Descargando news sentiment para {symbol}...\")\n",
    "    params = {\n",
    "        'function': 'NEWS_SENTIMENT',\n",
    "        'tickers': symbol,\n",
    "        'limit': 1000\n",
    "    }\n",
    "\n",
    "    data = fetch_data(params)\n",
    "\n",
    "    if data and \"feed\" in data and len(data[\"feed\"]) > 0:\n",
    "        df = pd.DataFrame(data[\"feed\"])\n",
    "        df.to_csv(os.path.join(news_dir, f\"{symbol}_news_sentiment.csv\"), index=False)\n",
    "        print(f\"‚úÖ Guardado correctamente: {symbol}_news_sentiment.csv\")\n",
    "    else:\n",
    "        print(f\"‚ùå No se encontraron datos v√°lidos para {symbol}\")\n",
    "\n",
    "    time.sleep(15)\n",
    "\n",
    "# Ejecutar solo esta descarga\n",
    "fetch_news_sentiment(\"GOOGL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9e136",
   "metadata": {},
   "source": [
    "### **News Extraction** - 2 Week-Window-Frame (03-2022 to 05-2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d68feb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, csv, time, requests, argparse\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "BASE_URL = \"https://www.alphavantage.co/query\"\n",
    "API_KEY  = os.getenv(\"ALPHA_VANTAGE_API_KEY\", \"TU_ALPHA_KEY\")\n",
    "\n",
    "def iso_param(dt: datetime) -> str:\n",
    "    return dt.strftime(\"%Y%m%dT%H%M\")\n",
    "\n",
    "def fetch_news(ticker, time_from, time_to, limit=1000):\n",
    "    params = {\n",
    "        \"function\":  \"NEWS_SENTIMENT\",\n",
    "        \"tickers\":   ticker,\n",
    "        \"apikey\":    API_KEY,\n",
    "        \"time_from\": time_from,\n",
    "        \"time_to\":   time_to,\n",
    "        \"limit\":     limit\n",
    "    }\n",
    "    r = requests.get(BASE_URL, params=params)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"feed\", [])\n",
    "\n",
    "def download_all(ticker, start_date, end_date, window_weeks, out_csv):\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt   = datetime.strptime(end_date,   \"%Y-%m-%d\")\n",
    "    window   = timedelta(weeks=window_weeks)\n",
    "    all_articles = []\n",
    "    current = start_dt\n",
    "\n",
    "    while current < end_dt:\n",
    "        nxt = min(current + window, end_dt)\n",
    "        tf = iso_param(current); tt = iso_param(nxt)\n",
    "        print(f\"Fetching {ticker} from {tf} to {tt}‚Ä¶\")\n",
    "        batch = fetch_news(ticker, tf, tt)\n",
    "        if batch:\n",
    "            all_articles.extend(batch)\n",
    "            print(f\"  ‚Üí {len(batch)} articles\")\n",
    "        else:\n",
    "            print(\"  ‚Üí no articles\")\n",
    "        current = nxt\n",
    "        time.sleep(12)\n",
    "\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    df.drop_duplicates(subset=[\"url\",\"time_published\"], inplace=True)\n",
    "    df.to_csv(out_csv, sep=';', index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "    print(f\"Saved {len(df)} unique articles to '{out_csv}'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a84eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Par√°metros\n",
    "    ticker     = \"META\"\n",
    "    start_date = \"2022-03-01\"\n",
    "    end_date   = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    window     = 1   # semanas\n",
    "    output_csv = f\"{ticker}_news_{start_date}_to_{end_date}.csv\"\n",
    "\n",
    "    # Llamada a la descarga\n",
    "    download_all(ticker, start_date, end_date, window, output_csv)\n",
    "    print(f\"‚úÖ Descarga completada para {ticker}, guardado en '{output_csv}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6828559",
   "metadata": {},
   "source": [
    "## 1.1. Data Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebbe2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "API_KEY = os.getenv(\"ALPHA_VANTAGE_API_KEY\")  # or replace with your key: \"YOUR_KEY_HERE\"\n",
    "SYMBOL = \"AAPL\"  # Change to your desired ticker\n",
    "df_to_enrich = df_expl.copy()  # <‚îÄ‚îÄ SELECT YOUR INPUT SENTIMENT DF HERE\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STEP 1: DOWNLOAD FUNDAMENTALS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "urls = {\n",
    "    \"income\": f\"https://www.alphavantage.co/query?function=INCOME_STATEMENT&symbol={SYMBOL}&apikey={API_KEY}\",\n",
    "    \"balance\": f\"https://www.alphavantage.co/query?function=BALANCE_SHEET&symbol={SYMBOL}&apikey={API_KEY}\",\n",
    "    \"cashflow\": f\"https://www.alphavantage.co/query?function=CASH_FLOW&symbol={SYMBOL}&apikey={API_KEY}\"\n",
    "}\n",
    "\n",
    "financial_reports = {}\n",
    "\n",
    "def fetch_quarterly_report(report_type):\n",
    "    print(f\"‚è≥ Downloading {report_type} report...\")\n",
    "    response = requests.get(urls[report_type])\n",
    "    data = response.json()\n",
    "    if \"quarterlyReports\" in data:\n",
    "        df = pd.DataFrame(data[\"quarterlyReports\"])\n",
    "        financial_reports[report_type] = df\n",
    "        print(f\"‚úÖ {report_type}: {df.shape[0]} quarters\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to load {report_type} report:\", data)\n",
    "\n",
    "for report in [\"income\", \"balance\", \"cashflow\"]:\n",
    "    fetch_quarterly_report(report)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STEP 2: CLEANING & RATIO CALCULATION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def first_available_column(df, candidates):\n",
    "    for col in candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    raise ValueError(f\"None of {candidates} found in DataFrame.\")\n",
    "\n",
    "def get_quarter_label(date):\n",
    "    month = date.month\n",
    "    quarter = (month - 1) // 3 + 1\n",
    "    return f\"{date.year}Q{quarter}\"\n",
    "\n",
    "def force_numeric(df):\n",
    "    for c in df.columns:\n",
    "        if c != \"fiscalDateEnding\":\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "df_income = financial_reports[\"income\"].copy()\n",
    "df_balance = financial_reports[\"balance\"].copy()\n",
    "df_cashflow = financial_reports[\"cashflow\"].copy()\n",
    "\n",
    "for df in [df_income, df_balance, df_cashflow]:\n",
    "    df[\"fiscalDateEnding\"] = pd.to_datetime(df[\"fiscalDateEnding\"])\n",
    "    df = force_numeric(df)\n",
    "\n",
    "df_income = df_income[df_income[\"fiscalDateEnding\"] >= \"2022-01-01\"]\n",
    "df_balance = df_balance[df_balance[\"fiscalDateEnding\"] >= \"2022-01-01\"]\n",
    "df_cashflow = df_cashflow[df_cashflow[\"fiscalDateEnding\"] >= \"2022-01-01\"]\n",
    "\n",
    "df = df_balance.merge(df_income, on=\"fiscalDateEnding\").merge(df_cashflow, on=\"fiscalDateEnding\")\n",
    "\n",
    "# Define fallback-safe columns\n",
    "net_income_col = first_available_column(df, [\"netIncome\", \"netIncomeFromContinuingOperations\", \"profitLoss\"])\n",
    "shares_col = first_available_column(df, [\"commonStockSharesOutstanding\"])\n",
    "\n",
    "# Financial ratios\n",
    "df[\"current_ratio\"] = df[\"totalCurrentAssets\"] / df[\"totalCurrentLiabilities\"]\n",
    "df[\"quick_ratio\"] = (df[\"cashAndShortTermInvestments\"] + df[\"currentNetReceivables\"]) / df[\"totalCurrentLiabilities\"]\n",
    "df[\"debt_to_assets\"] = df[\"totalLiabilities\"] / df[\"totalAssets\"]\n",
    "df[\"debt_to_equity\"] = df[\"totalLiabilities\"] / df[\"totalShareholderEquity\"]\n",
    "df[\"gross_margin\"] = df[\"grossProfit\"] / df[\"totalRevenue\"]\n",
    "df[\"operating_margin\"] = df[\"operatingIncome\"] / df[\"totalRevenue\"]\n",
    "df[\"net_profit_margin\"] = df[net_income_col] / df[\"totalRevenue\"]\n",
    "df[\"return_on_assets\"] = df[net_income_col] / df[\"totalAssets\"]\n",
    "df[\"return_on_equity\"] = df[net_income_col] / df[\"totalShareholderEquity\"]\n",
    "df[\"operating_cash_flow_ratio\"] = df[\"operatingCashflow\"] / df[\"totalCurrentLiabilities\"]\n",
    "df[\"free_cash_flow\"] = df[\"operatingCashflow\"] - df[\"capitalExpenditures\"]\n",
    "df[\"book_value_per_share\"] = df[\"totalShareholderEquity\"] / df[shares_col]\n",
    "df[\"revenue_per_share\"] = df[\"totalRevenue\"] / df[shares_col]\n",
    "df[\"eps\"] = df[net_income_col] / df[shares_col]\n",
    "df[\"quarter\"] = df[\"fiscalDateEnding\"].apply(get_quarter_label)\n",
    "\n",
    "df_ratios = df[[\n",
    "    \"quarter\", \"eps\", \"book_value_per_share\", \"revenue_per_share\",\n",
    "    \"current_ratio\", \"quick_ratio\", \"debt_to_assets\", \"debt_to_equity\",\n",
    "    \"gross_margin\", \"operating_margin\", \"net_profit_margin\",\n",
    "    \"return_on_assets\", \"return_on_equity\", \"operating_cash_flow_ratio\", \"free_cash_flow\"\n",
    "]]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STEP 3: LOAD WEEKLY PRICES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "price_path = f\"{BASE_PATH}/data/extract/stocks/{SYMBOL}/prices/{SYMBOL}_weekly_adjusted.csv\"\n",
    "df_price = pd.read_csv(price_path, usecols=[\"date\", \"5. adjusted close\"])\n",
    "df_price[\"date\"] = pd.to_datetime(df_price[\"date\"])\n",
    "df_price = df_price[df_price[\"date\"] >= \"2022-01-01\"].sort_values(\"date\")\n",
    "df_price = df_price.rename(columns={\"5. adjusted close\": \"adjusted_close\"})\n",
    "df_price[\"quarter\"] = df_price[\"date\"].apply(get_quarter_label)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STEP 4: VALUATION RATIOS & ENRICHMENT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df_final = df_price.merge(df_ratios, on=\"quarter\", how=\"left\")\n",
    "df_final[\"PER\"] = df_final[\"adjusted_close\"] / df_final[\"eps\"]\n",
    "df_final[\"PB_ratio\"] = df_final[\"adjusted_close\"] / df_final[\"book_value_per_share\"]\n",
    "df_final[\"PS_ratio\"] = df_final[\"adjusted_close\"] / df_final[\"revenue_per_share\"]\n",
    "df_final[\"week\"] = df_final[\"date\"].dt.to_period(\"W\").apply(lambda r: r.start_time)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STEP 5: MERGE WITH NEWS SENTIMENT DF ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df_to_enrich[\"time_published\"] = pd.to_datetime(df_to_enrich[\"time_published\"])\n",
    "df_to_enrich[\"week\"] = df_to_enrich[\"time_published\"].dt.to_period(\"W\").apply(lambda r: r.start_time)\n",
    "df_merged = pd.merge(df_to_enrich, df_final.drop(columns=[\"date\"]), on=\"week\", how=\"left\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STEP 6: EXPORT AND PREVIEW ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "output_file = f\"{SYMBOL}_enriched_sentiment_with_fundamentals.csv\"\n",
    "df_merged.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Final enriched DataFrame saved as {output_file}\")\n",
    "df_merged.head(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvMasterThesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
